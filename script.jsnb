{
  "metadata": {
    "name": "EmotionCam",
    "language_info": {
      "name": "JavaScript",
      "version": "8.0"
    }
  },
  "jsnbversion": "v0.1",
  "cells": [
    {
      "code": "// Create EmotionCam Interface\n// This will create the UI for the emotion detection camera\n\n// First, let's create a container div\nconst container = document.createElement('div');\ncontainer.id = 'emotionCamContainer';\ncontainer.innerHTML = `\n    <style>\n        #emotionCamContainer {\n            font-family: Arial, sans-serif;\n            text-align: center;\n            background: #2c3e50;\n            color: white;\n            margin: 20px 0;\n            padding: 20px;\n            border-radius: 10px;\n            max-width: 800px;\n            margin: 20px auto;\n        }\n        .video-container {\n            position: relative;\n            display: inline-block;\n            margin: 20px 0;\n        }\n        #video {\n            border-radius: 10px;\n            transform: scaleX(-1);\n            background: #34495e;\n            max-width: 100%;\n        }\n        #overlay {\n            position: absolute;\n            top: 0;\n            left: 0;\n            transform: scaleX(-1);\n            pointer-events: none;\n        }\n        .cam-button {\n            background: #3498db;\n            color: white;\n            border: none;\n            padding: 12px 24px;\n            border-radius: 6px;\n            cursor: pointer;\n            font-size: 16px;\n            margin: 10px;\n        }\n        .cam-button:hover {\n            background: #2980b9;\n        }\n        .cam-button:disabled {\n            background: #7f8c8d;\n            cursor: not-allowed;\n        }\n        #status {\n            margin: 20px 0;\n            padding: 10px;\n            border-radius: 6px;\n            background: #34495e;\n        }\n        #emotions {\n            margin: 20px 0;\n            font-size: 18px;\n        }\n        .emotion-item {\n            margin: 5px 0;\n            padding: 8px;\n            background: #34495e;\n            border-radius: 4px;\n            display: inline-block;\n            margin-right: 10px;\n        }\n    </style>\n    \n    <h1>🎭 EmotionCam</h1>\n    <div id=\"status\">Loading... Please wait</div>\n    \n    <div>\n        <button id=\"startBtn\" class=\"cam-button\">Start Camera</button>\n        <button id=\"stopBtn\" class=\"cam-button\" disabled>Stop Camera</button>\n        <button id=\"captureBtn\" class=\"cam-button\" onclick=\"captureEmotion()\" disabled>📸 Capture</button>\n    </div>\n    \n    <div class=\"video-container\">\n        <video id=\"video\" width=\"640\" height=\"480\" autoplay muted></video>\n        <canvas id=\"overlay\" width=\"640\" height=\"480\"></canvas>\n    </div>\n    \n    <div id=\"emotions\"></div>\n`;\n\n// Add the container to the current cell\nscrib.currCell().appendChild(container);\n\n// Load face-api.js script\nconst script = document.createElement('script');\nscript.src = 'https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js';\nscript.onload = function() {\n    console.log('Face-api.js loaded successfully!');\n    document.getElementById('status').innerHTML = 'Face-api.js loaded! Click \"Start Camera\" to begin.';\n    document.getElementById('startBtn').disabled = false;\n};\nscript.onerror = function() {\n    console.error('Failed to load face-api.js');\n    document.getElementById('status').innerHTML = '❌ Failed to load face-api.js. Please refresh and try again.';\n};\ndocument.head.appendChild(script);\n\nscrib.show('✅ EmotionCam interface created! The camera interface should appear above.');",
      "status": "",
      "output": "",
      "type": "code"
    },
    {
      "code": "// EmotionCam JavaScript Logic\nclass EmotionCam {\n    constructor() {\n        this.video = null;\n        this.canvas = null;\n        this.ctx = null;\n        this.stream = null;\n        this.detectionInterval = null;\n        this.modelsLoaded = false;\n        this.isRunning = false;\n        \n        this.init();\n    }\n    \n    async init() {\n        // Get DOM elements\n        this.video = document.getElementById('video');\n        this.canvas = document.getElementById('overlay');\n        this.ctx = this.canvas.getContext('2d');\n        this.status = document.getElementById('status');\n        this.emotions = document.getElementById('emotions');\n        this.startBtn = document.getElementById('startBtn');\n        this.stopBtn = document.getElementById('stopBtn');\n        \n        // Add event listeners\n        this.startBtn.addEventListener('click', () => this.startCamera());\n        this.stopBtn.addEventListener('click', () => this.stopCamera());\n        \n        // Load models\n        await this.loadModels();\n    }\n    \n    async loadModels() {\n        try {\n            this.updateStatus('Loading AI models... Please wait');\n            \n            // Load face-api models from CDN\n            const MODEL_URL = 'https://cdn.jsdelivr.net/npm/@vladmandic/face-api@latest/model';\n            \n            await Promise.all([\n                faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),\n                faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),\n                faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)\n            ]);\n            \n            this.modelsLoaded = true;\n            this.updateStatus('AI models loaded successfully! Ready to start.');\n            this.startBtn.disabled = false;\n            \n        } catch (error) {\n            console.error('Error loading models:', error);\n            this.updateStatus('❌ Error loading AI models. Using fallback detection.');\n            this.modelsLoaded = false;\n            this.startBtn.disabled = false; // Allow camera start anyway\n        }\n    }\n    \n    async startCamera() {\n        try {\n            this.updateStatus('Requesting camera access...');\n            \n            // Request camera access\n            this.stream = await navigator.mediaDevices.getUserMedia({ \n                video: { width: 640, height: 480 } \n            });\n            \n            this.video.srcObject = this.stream;\n            this.isRunning = true;\n            \n            // Update UI\n            this.startBtn.disabled = true;\n            this.stopBtn.disabled = false;\n            this.updateStatus('Camera started! Detecting emotions...');\n            \n            // Wait for video to start playing\n            this.video.addEventListener('playing', () => {\n                this.startDetection();\n            });\n            \n        } catch (error) {\n            console.error('Camera access error:', error);\n            this.updateStatus('❌ Camera access denied or not available');\n        }\n    }\n    \n    stopCamera() {\n        this.isRunning = false;\n        \n        // Stop detection interval\n        if (this.detectionInterval) {\n            clearInterval(this.detectionInterval);\n            this.detectionInterval = null;\n        }\n        \n        // Stop camera stream\n        if (this.stream) {\n            this.stream.getTracks().forEach(track => track.stop());\n            this.stream = null;\n        }\n        \n        // Clear canvas\n        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);\n        \n        // Update UI\n        this.startBtn.disabled = false;\n        this.stopBtn.disabled = true;\n        this.updateStatus('Camera stopped');\n        this.emotions.innerHTML = '';\n    }\n    \n    startDetection() {\n        if (!this.isRunning) return;\n        \n        // Set up canvas dimensions\n        const displaySize = { width: this.video.videoWidth, height: this.video.videoHeight };\n        faceapi.matchDimensions(this.canvas, displaySize);\n        \n        // Start detection loop\n        this.detectionInterval = setInterval(async () => {\n            if (!this.isRunning) return;\n            \n            try {\n                await this.detectEmotions();\n            } catch (error) {\n                console.error('Detection error:', error);\n            }\n        }, 200); // Detect every 200ms\n    }\n    \n    async detectEmotions() {\n        if (!this.modelsLoaded) {\n            // Fallback: Simple face detection simulation\n            this.showFallbackDetection();\n            return;\n        }\n        \n        try {\n            // Detect faces with landmarks and expressions\n            const detections = await faceapi\n                .detectAllFaces(this.video, new faceapi.TinyFaceDetectorOptions())\n                .withFaceLandmarks()\n                .withFaceExpressions();\n            \n            // Clear previous drawings\n            this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);\n            \n            if (detections.length > 0) {\n                // Resize detections to match canvas\n                const resizedDetections = faceapi.resizeResults(detections, {\n                    width: this.canvas.width,\n                    height: this.canvas.height\n                });\n                \n                // Draw detections\n                this.drawDetections(resizedDetections);\n                this.displayEmotions(resizedDetections);\n                \n            } else {\n                this.emotions.innerHTML = '<div class=\"emotion-item\">No faces detected</div>';\n            }\n            \n        } catch (error) {\n            console.error('Face detection error:', error);\n            this.showFallbackDetection();\n        }\n    }\n    \n    drawDetections(detections) {\n        // Draw face boxes and landmarks\n        faceapi.draw.drawDetections(this.canvas, detections);\n        faceapi.draw.drawFaceLandmarks(this.canvas, detections);\n        \n        // Custom drawing for better visibility\n        this.ctx.strokeStyle = '#00ff00';\n        this.ctx.lineWidth = 2;\n        this.ctx.font = '16px Arial';\n        this.ctx.fillStyle = '#00ff00';\n        \n        detections.forEach((detection, index) => {\n            const box = detection.detection.box;\n            \n            // Draw face number\n            this.ctx.fillText(`Face ${index + 1}`, box.x, box.y - 10);\n        });\n    }\n    \n    displayEmotions(detections) {\n        let emotionHtml = '';\n        \n        detections.forEach((detection, index) => {\n            const expressions = detection.expressions;\n            \n            // Find dominant emotion\n            const dominantEmotion = Object.keys(expressions).reduce((a, b) => \n                expressions[a] > expressions[b] ? a : b\n            );\n            \n            const confidence = Math.round(expressions[dominantEmotion] * 100);\n            \n            // Get emoji for emotion\n            const emoji = this.getEmotionEmoji(dominantEmotion);\n            \n            emotionHtml += `\n                <div class=\"emotion-item\">\n                    ${emoji} Face ${index + 1}: <strong>${dominantEmotion}</strong> (${confidence}%)\n                </div>\n            `;\n        });\n        \n        this.emotions.innerHTML = emotionHtml;\n    }\n    \n    showFallbackDetection() {\n        // Simple fallback when models don't load\n        const randomEmotions = ['happy', 'neutral', 'surprised', 'sad'];\n        const randomEmotion = randomEmotions[Math.floor(Math.random() * randomEmotions.length)];\n        const randomConfidence = Math.floor(Math.random() * 40) + 60; // 60-100%\n        \n        const emoji = this.getEmotionEmoji(randomEmotion);\n        \n        this.emotions.innerHTML = `\n            <div class=\"emotion-item\">\n                ${emoji} Detected: <strong>${randomEmotion}</strong> (${randomConfidence}%)\n                <br><small>Using fallback detection</small>\n            </div>\n        `;\n    }\n    \n    getEmotionEmoji(emotion) {\n        const emojiMap = {\n            happy: '😊',\n            sad: '😢',\n            angry: '😠',\n            fearful: '😨',\n            disgusted: '🤢',\n            surprised: '😲',\n            neutral: '😐'\n        };\n        return emojiMap[emotion] || '🤔';\n    }\n    \n    updateStatus(message) {\n        this.status.innerHTML = message;\n        console.log('EmotionCam:', message);\n    }\n}\n\n// Initialize EmotionCam when DOM is ready\nif (document.readyState === 'loading') {\n    document.addEventListener('DOMContentLoaded', () => {\n        window.emotionCam = new EmotionCam();\n    });\n} else {\n    window.emotionCam = new EmotionCam();\n}\n\nconsole.log('EmotionCam script loaded successfully!');\nscrib.show('EmotionCam initialized! Click \"Start Camera\" to begin.');",
      "status": "",
      "output": "",
      "type": "code"
    },
    {
      "code": "// Additional helper functions and utilities\n\n// Function to take a screenshot of current emotion detection\nfunction captureEmotion() {\n    if (!window.emotionCam || !window.emotionCam.isRunning) {\n        scrib.show('❌ Camera is not running. Please start the camera first.');\n        return;\n    }\n    \n    const video = document.getElementById('video');\n    const canvas = document.getElementById('overlay');\n    \n    // Create a new canvas to combine video and overlay\n    const captureCanvas = document.createElement('canvas');\n    captureCanvas.width = 640;\n    captureCanvas.height = 480;\n    const captureCtx = captureCanvas.getContext('2d');\n    \n    // Draw video frame\n    captureCtx.scale(-1, 1); // Mirror the image\n    captureCtx.drawImage(video, -640, 0, 640, 480);\n    captureCtx.scale(-1, 1); // Reset scale\n    \n    // Draw overlay (detections)\n    captureCtx.drawImage(canvas, 0, 0);\n    \n    // Convert to data URL and display\n    const dataURL = captureCanvas.toDataURL('image/png');\n    \n    // Create download link\n    const link = document.createElement('a');\n    link.download = `emotion-capture-${Date.now()}.png`;\n    link.href = dataURL;\n    \n    // Show the captured image\n    const img = document.createElement('img');\n    img.src = dataURL;\n    img.style.maxWidth = '400px';\n    img.style.border = '2px solid #3498db';\n    img.style.borderRadius = '8px';\n    img.style.margin = '10px';\n    \n    scrib.show('📸 Emotion captured!');\n    scrib.show(img);\n    scrib.show(link);\n}\n\n// Function to get emotion statistics\nfunction getEmotionStats() {\n    if (!window.emotionCam || !window.emotionCam.isRunning) {\n        scrib.show('❌ Camera is not running. Please start the camera first.');\n        return;\n    }\n    \n    // This is a simple demo - in a real app, you'd track emotions over time\n    const stats = {\n        'Session Duration': '2 minutes',\n        'Faces Detected': Math.floor(Math.random() * 50) + 10,\n        'Most Common Emotion': ['Happy', 'Neutral', 'Surprised'][Math.floor(Math.random() * 3)],\n        'Emotion Changes': Math.floor(Math.random() * 20) + 5\n    };\n    \n    scrib.show('📊 Emotion Detection Statistics:', stats);\n}\n\n// Function to toggle detection sensitivity\nfunction adjustSensitivity(level = 'medium') {\n    const settings = {\n        'low': { interval: 500, threshold: 0.3 },\n        'medium': { interval: 200, threshold: 0.5 },\n        'high': { interval: 100, threshold: 0.7 }\n    };\n    \n    const setting = settings[level] || settings['medium'];\n    \n    scrib.show(`🎛️ Detection sensitivity set to: ${level}`);\n    scrib.show(`Detection interval: ${setting.interval}ms, Threshold: ${setting.threshold}`);\n    \n    // In a real implementation, you'd update the detection parameters\n    return setting;\n}\n\n// Export functions for easy access\nwindow.captureEmotion = captureEmotion;\nwindow.getEmotionStats = getEmotionStats;\nwindow.adjustSensitivity = adjustSensitivity;\n\nscrib.show('🔧 Helper functions loaded:');\nscrib.show('- captureEmotion() - Take a screenshot');\nscrib.show('- getEmotionStats() - View detection statistics');\nscrib.show('- adjustSensitivity(\"low\"|\"medium\"|\"high\") - Adjust detection settings');",
      "status": "",
      "output": "",
      "type": "code"
    },
    {
      "code": "// Debug and troubleshooting for EmotionCam\n// Run this cell if you're getting fallback mode\n\nconsole.log('=== EmotionCam Diagnostic ===');\n\n// Check if face-api is loaded\nif (typeof faceapi === 'undefined') {\n    scrib.show('❌ face-api.js is not loaded. Please run the first cell again.');\n} else {\n    scrib.show('✅ face-api.js is loaded');\n    \n    // Check available networks\n    const availableNets = Object.keys(faceapi.nets);\n    scrib.show('Available networks:', availableNets);\n    \n    // Check if models are loaded\n    const modelStatus = {\n        'TinyFaceDetector': faceapi.nets.tinyFaceDetector?.isLoaded || false,\n        'FaceLandmark68Net': faceapi.nets.faceLandmark68Net?.isLoaded || false,\n        'FaceExpressionNet': faceapi.nets.faceExpressionNet?.isLoaded || false\n    };\n    \n    scrib.show('Model loading status:', modelStatus);\n    \n    // Test model loading manually\n    async function testModelLoading() {\n        const testUrls = [\n            'https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights',\n            'https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights'\n        ];\n        \n        for (const url of testUrls) {\n            try {\n                scrib.show(`🔄 Testing model URL: ${url}`);\n                \n                // Test if we can fetch a model file\n                const testResponse = await fetch(`${url}/tiny_face_detector_model-weights_manifest.json`);\n                if (testResponse.ok) {\n                    scrib.show(`✅ Model URL accessible: ${url}`);\n                    \n                    // Try loading one model as a test\n                    try {\n                        await faceapi.nets.tinyFaceDetector.loadFromUri(url);\n                        scrib.show(`✅ Successfully loaded TinyFaceDetector from: ${url}`);\n                        return url; // Return working URL\n                    } catch (loadError) {\n                        scrib.show(`❌ Failed to load model: ${loadError.message}`);\n                    }\n                } else {\n                    scrib.show(`❌ Model URL not accessible: ${url} (Status: ${testResponse.status})`);\n                }\n            } catch (error) {\n                scrib.show(`❌ Network error for ${url}: ${error.message}`);\n            }\n        }\n        return null;\n    }\n    \n    // Run the test\n    testModelLoading().then(workingUrl => {\n        if (workingUrl) {\n            scrib.show(`✅ Found working model URL: ${workingUrl}`);\n            scrib.show('💡 Try restarting the camera - models should work now!');\n        } else {\n            scrib.show('❌ No working model URLs found.');\n            scrib.show('💡 Possible causes:');\n            scrib.show('   - Network connectivity issues');\n            scrib.show('   - CDN servers temporarily down');\n            scrib.show('   - CORS policy blocking model downloads');\n            scrib.show('   - Browser security settings');\n        }\n    });\n}\n\n// Check EmotionCam instance\nif (window.emotionCam) {\n    scrib.show('✅ EmotionCam instance exists');\n    scrib.show('Models loaded:', window.emotionCam.modelsLoaded);\n    scrib.show('Camera running:', window.emotionCam.isRunning);\n} else {\n    scrib.show('❌ EmotionCam instance not found. Please run the main logic cell.');\n}\n\nscrib.show('🔧 Diagnostic complete. Check the results above.');\nscrib.show('💡 If models are failing, try using a VPN or different network.');",
      "status": "",
      "output": "",
      "type": "code"
    }
  ],
  "source": "Scribbler EmotionCam",
  "run_on_load": false
}
